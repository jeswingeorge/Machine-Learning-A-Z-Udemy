{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [An End-to-End Guide to Understand the Math behind XGBoost](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)\n",
    "\n",
    "The beauty of this powerful algorithm lies in its scalability(i.e, works effectively on different sizes of data), which drives fast learning through parallel and distributed computing and offers efficient memory usage.\n",
    "\n",
    "#### Why ensemble learning?\n",
    "XGBoost is an ensemble learning method. Sometimes, it may not be sufficient to rely upon the results of just one machine learning model. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models.\n",
    "\n",
    "__The models that form the ensemble, also known as base learners, could be either from the same learning algorithm or different learning algorithms. Bagging and boosting are two widely used ensemble learners.__\n",
    "\n",
    "##### Bagging\n",
    "\n",
    "While decision trees are one of the most easily interpretable models, they exhibit highly variable behavior. Consider a single training dataset that we randomly split into two parts. Now, let’s use each part to train a decision tree in order to obtain two models.\n",
    "\n",
    "When we fit both these models, they would yield different results. Decision trees are said to be associated with high variance due to this behavior. Bagging or boosting aggregation helps to reduce the variance in any learner. Several decision trees which are generated in parallel, form the base learners of bagging technique. Data sampled with replacement is fed to these learners for training. The final prediction is the averaged output from all the learners.\n",
    "\n",
    "##### Boosting\n",
    "In boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. Each tree learns from its predecessors and updates the residual errors. Hence, the tree that grows next in the sequence will learn from an updated version of the residuals.\n",
    "\n",
    "The base learners in boosting are weak learners in which the bias is high, and the predictive power is just a tad better than random guessing. Each of these weak learners contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners. The final strong learner brings down both the bias and the variance.\n",
    "\n",
    "\n",
    "__In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits.__ Such small trees, which are not very deep, are highly interpretable. Parameters like the number of trees or iterations, the rate at which the gradient boosting learns, and the depth of the tree, could be optimally selected through validation techniques like k-fold cross validation. __Having a large number of trees might lead to overfitting. So, it is necessary to carefully choose the stopping criteria for boosting.__\n",
    "\n",
    "The boosting ensemble technique consists of three simple steps:\n",
    "\n",
    "- An initial model F0 is defined to predict the target variable y. This model will be associated with a residual (y – F0)\n",
    "- A new model h1 is fit to the residuals from the previous step\n",
    "- Now, F0 and h1 are combined to give F1, the boosted version of F0. The mean squared error from F1 will be lower than that from F0:\n",
    "\n",
    "$$ F_1(x) <- F_0(x) + h_1(x) $$\n",
    "\n",
    "- To improve the performance of F1, we could model after the residuals of F1 and create a new model F2:\n",
    "\n",
    "$$ F_2(x) <- F_1(x) + h_2(x) $$\n",
    "\n",
    "- This can be done for ‘m’ iterations, until residuals have been minimized as much as possible:\n",
    "\n",
    "$$ F_m(x) <- F_{m-1}(x) + h_m(x) $$\n",
    "\n",
    "- Here, the additive learners do not disturb the functions created in the previous steps. Instead, they impart information of their own to bring down the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Demonstrating the Potential of Boosting\n",
    "\n",
    "Consider the following data where the years of experience is predictor variable and salary (in thousand dollars) is the target. Using regression trees as base learners, we can create an ensemble model to predict the salary. For the sake of simplicity, we can choose square loss as our loss function and our objective would be to minimize the square error.\n",
    "\n",
    "![](images/3.PNG)\n",
    "\n",
    "As the first step, the model should be initialized with a function F0(x). F0(x) should be a function which minimizes the loss function or MSE (mean squared error), in this case:\n",
    "\n",
    "$$ F_{0}(x) = argmin_{\\gamma}\\sum_{i=1}^nL(y_i,\\gamma) $$\n",
    "\n",
    "$$ argmin_{\\gamma}\\sum_{i=1}^nL(y_i,\\gamma) = argmin_{\\gamma}\\sum_{i=1}^n(y_i - \\gamma) $$\n",
    "\n",
    "Taking the first differential of the above equation with respect to $\\gamma$, it is seen that the function minimizes at the mean $\\sum_{i=1}^ny_i$. So, the boosting model could be initiated with:\n",
    "\n",
    "$$ F_{0}(x) = \\frac{\\sum_{i=1}^ny_i}{n} $$\n",
    "\n",
    "F0(x) gives the predictions from the first stage of our model. Now, the residual error for each instance is (yi – F0(x)).\n",
    "\n",
    "![](images/4.PNG)\n",
    "\n",
    "We can use the residuals from F0(x) to create h1(x). h1(x) will be a regression tree which will try and reduce the residuals from the previous step. The output of h1(x) won’t be a prediction of y; instead, it will help in predicting the successive function F1(x) which will bring down the residuals.\n",
    "\n",
    "![](images/5.PNG)\n",
    "\n",
    "The additive model h1(x) computes the mean of the residuals (y – F0) at each leaf of the tree. The boosted function F1(x) is obtained by summing F0(x) and h1(x). This way h1(x) learns from the residuals of F0(x) and suppresses it in F1(x).\n",
    "\n",
    "![](images/6.PNG)\n",
    "\n",
    "This can be repeated for 2 more iterations to compute $h_2(x)$ and $h_3(x)$. Each of these additive learners, h_m(x), will make use of the residuals from the preceding function, $F_{m-1}(x)$.\n",
    "\n",
    "![](images/7.PNG)\n",
    "\n",
    "![](images/8.PNG)\n",
    "\n",
    "The MSEs for F0(x), F1(x) and F2(x) are 875, 692 and 540. It’s amazing how these simple weak learners can bring about a huge reduction in error!\n",
    "\n",
    "Note that each learner, hm(x), is trained on the residuals. All the additive learners in boosting are modeled after the residual errors at each step. Intuitively, it could be observed that the boosting learners make use of the patterns in residual errors. At the stage where maximum accuracy is reached by boosting, the residuals appear to be randomly distributed without any pattern.\n",
    "\n",
    "***\n",
    "## Using gradient descent for optimizing the loss function\n",
    "\n",
    "In the case discussed above, MSE was the loss function. The mean minimized the error here. When MAE (mean absolute error) is the loss function, the median would be used as F0(x) to initialize the model. A unit change in y would cause a unit change in MAE as well.\n",
    "\n",
    "For MSE, the change observed would be roughly exponential. Instead of fitting hm(x) on the residuals, fitting it on the gradient of loss function, or the step along which loss occurs, would make this process generic and applicable across all loss functions.\n",
    "\n",
    "Gradient descent helps us minimize any differentiable function. Earlier, the regression tree for hm(x) predicted the mean residual at each terminal node of the tree. In gradient boosting, the average gradient component would be computed.\n",
    "\n",
    "For each node, there is a factor γ with which hm(x) is multiplied. This accounts for the difference in impact of each branch of the split. Gradient boosting helps in predicting the optimal gradient for the additive model, unlike classical gradient descent techniques which reduce error in the output at each iteration.\n",
    "\n",
    "The following steps are involved in gradient boosting:\n",
    "\n",
    "- F0(x) – with which we initialize the boosting algorithm – is to be defined:\n",
    "\n",
    "$$ F_0(x) = argmin_{\\gamma}\\sum_i^nL(y_i,\\gamma)$$\n",
    "\n",
    "- The gradient of the loss function is computed iteratively:\n",
    "\n",
    "$$ r_{im} = -\\alpha[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x) = F_{m-1}(x)} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "- Each hm(x) is fit on the gradient obtained at each step\n",
    "- The multiplicative factor $\\gamma_m$ for each terminal node is derived and the boosted model Fm(x) is defined:\n",
    "$$ F_m(x) = F_{m-1}(x) + \\gamma_mh_m()x $$\n",
    "\n",
    "***\n",
    "\n",
    "### Unique features of XGBoost\n",
    "XGBoost is a popular implementation of gradient boosting. Let’s discuss some features of XGBoost that make it so interesting.\n",
    "\n",
    "- __Regularization:__ XGBoost has an option to penalize complex models through both L1 and L2 regularization. Regularization helps in preventing overfitting\n",
    "- __Handling sparse data:__ Missing values or data processing steps like one-hot encoding make data sparse. XGBoost incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data\n",
    "- __Weighted quantile sketch:__ Most existing tree based algorithms can find the split points when the data points are of equal weights (using quantile sketch algorithm). However, they are not equipped to handle weighted data. XGBoost has a distributed weighted quantile sketch algorithm to effectively handle weighted data\n",
    "- __Block structure for parallel learning:__ For faster computing, XGBoost can make use of multiple cores on the CPU. This is possible because of a block structure in its system design. Data is sorted and stored in in-memory units called blocks. Unlike other algorithms, this enables the data layout to be reused by subsequent iterations, instead of computing it again. This feature also serves useful for steps like split finding and column sub-sampling\n",
    "- __Cache awareness:__ In XGBoost, non-continuous memory access is required to get the gradient statistics by row index. Hence, XGBoost has been designed to make optimal use of hardware. This is done by allocating internal buffers in each thread, where the gradient statistics can be stored\n",
    "- __Out-of-core computing:__ This feature optimizes the available disk space and maximizes its usage when handling huge datasets that do not fit into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
