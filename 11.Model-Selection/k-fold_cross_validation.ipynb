{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will do 2 things:\n",
    "1. Evaluating our model performance\n",
    "2. Improving our model performance\n",
    "\n",
    "Improving the model performance can be done with technique called __Model Selection__ that consists of choosing the best parameters of your machine learning models. Every time we built a machine learning model we have two types of parameters:\n",
    "- Parameters that were changed and optimal values for them was found by running the model\n",
    "- Parameters that we choose ourselves.\n",
    "\n",
    "***\n",
    "\n",
    "__Reference:__\n",
    "[What is the Difference Between a Parameter and a Hyperparameter?](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
    "\n",
    "A __model parameter__ is a configuration variable that is internal to the model and whose value can be estimated from data. They are often not set manually by the practitioner. Some examples of model parameters include:\n",
    "\n",
    "- The weights in an artificial neural network.\n",
    "- The support vectors in a support vector machine.\n",
    "- The coefficients in a linear regression or logistic regression.\n",
    "\n",
    "A __model hyperparameter__ is a configuration that is external to the model and whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem.\n",
    "\n",
    "We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n",
    "\n",
    "When a machine learning algorithm is tuned for a specific problem, such as when you are using a grid search or a random search, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions.\n",
    "\n",
    "Some examples of model hyperparameters include:\n",
    "- The learning rate for training a neural network.\n",
    "- The C and sigma hyperparameters for support vector machines.\n",
    "- The k in k-nearest neighbors.\n",
    "\n",
    " A good rule of thumb to overcome this confusion is as follows:\n",
    "> If you have to specify a model parameter manually then it is probably a model hyperparameter.\n",
    "\n",
    "***\n",
    "\n",
    "Grid Search helps us choose the model hyperparameters.\n",
    "\n",
    "Just by checking accuracy of our model on one test data we cannot be sure of the models performance and accuracy. Also the variance problem.\n",
    "\n",
    "And so there is a technique called __k-Fold cross validation__ that improves this a lot becasue that will fix this variance problem.\n",
    "\n",
    "__k-Fold cross validation__ will split the training set into 10 folds (when k=10 => __10-fold cross-validation__) and we train our model on 9 folds and test on the last remaing fold. So we can get 10 combinations of data and in each combination we will have 9 folds to train the model and 1 to test it. This ensures that every observation from the original dataset has the chance of appearing in training and test set.\n",
    "\n",
    "> Usaually k = 10 is preferred it has been found through experimentation to generally result in a model with low bias a modest variance.\n",
    "\n",
    "1. Split the entire data randomly into k folds (value of k shouldn’t be too small or too high, ideally we choose 5 to 10 depending on the data size). The higher value of K leads to less biased model (but large variance might lead to overfit), where as the lower value of K is similar to the train-test split approach we saw before.\n",
    "2. Then fit the model using the K — 1 (K minus 1) folds and validate the model using the remaining Kth fold. Note down the scores/errors.\n",
    "3. Repeat this process until every K-fold serve as the test set. Then take the average of your recorded scores. That will be the performance metric for the model.\n",
    "\n",
    "\n",
    "![](k1.png)\n",
    "\n",
    "__Then we can take average of the accuracy of the 10 evaluations and also compute the standard deviation to look at the variance. And we can know in which of the four category of bulls-eye diagram our model will be.__\n",
    "\n",
    "###  bulls-eye diagram\n",
    "\n",
    "![](k2.png)\n",
    "\n",
    "\n",
    "In the above diagram, center of the target is a model that perfectly predicts correct values. As we move away from the bulls-eye our predictions become get worse and worse. We can repeat our process of model building to get separate hits on the target.\n",
    "\n",
    "So our model will be more relevant.\n",
    "\n",
    "The most relevant position for the k-fold cross validation is right after we have build the model. \n",
    "\n",
    "\n",
    "(trade-off: a balance achieved between two desirable but incompatible features)\n",
    "\n",
    "In supervised learning, __underfitting__ happens when a model unable to capture the underlying pattern of the data. These models usually have high bias and low variance. It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression.\n",
    "\n",
    "\n",
    "In supervised learning, __overfitting__ happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. These models have low bias and high variance. These models are very complex like Decision trees which are prone to overfitting.\n",
    "\n",
    "### What is Bias Variance Tradeoff?\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "#### Total Error\n",
    "To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.\n",
    "\n",
    "![](k3.PNG)\n",
    "\n",
    "An optimal balance of bias and variance would never overfit or underfit the model.  \n",
    "\n",
    "Therefore understanding bias and variance is critical for understanding the behavior of prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T06:15:26.814373Z",
     "start_time": "2020-04-16T06:15:04.617428Z"
    }
   },
   "outputs": [],
   "source": [
    "# k-Fold Cross Validation\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn k-Fold Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "__cv__ parameter is the number of folds that we want to split the training set into.\n",
    "\n",
    "We can write a logic manually to perform this or we can use the built in `cross_val_score` (returns score of each test folds)/`cross_val_predict` (returns the predicted score for each observation in the input dataset when it was part of the test set) from the scikit_learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T06:15:27.087350Z",
     "start_time": "2020-04-16T06:15:26.829343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8       , 0.96666667, 0.8       , 0.96666667, 0.86666667,\n",
       "       0.86666667, 0.9       , 0.93333333, 1.        , 0.93333333])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "# accuracies is a vector that will get the 10 accuracies computed through k-fold cross validation\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T06:15:27.108336Z",
     "start_time": "2020-04-16T06:15:27.097344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9033333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T06:15:27.313530Z",
     "start_time": "2020-04-16T06:15:27.116331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06574360974438671"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std() # we get 6% std deviation - low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always a good practice to see both mean and standard deviation of cv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of Train, Validation, and Test Datasets\n",
    "\n",
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "> k-fold cross validation is an excellent way to calculate an unbiased estimate of the skill of your model on unseen data.\n",
    "\n",
    "\n",
    "> If the esitmator (model) is a claissifier and ‘y’(target variable) is either binary/multicalss, then __StratifiedKfold__ technique is used by default. In all other cases __K_Fold__ technique is used as a default to split and train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 Reasons why you should use Cross-Validation in your Data Science Projects](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)\n",
    "\n",
    "1. __Use All Your Data__: When we use cross-validation, we build K different models, so we are able to make predictions on all of our data.\n",
    "\n",
    "2. __Get More Metrics:__ when we create five different models using our learning algorithm and test it on five different test sets, we can be more confident in our algorithm performance. When we do a single evaluation on our test set, we get only one result. This result may be because of chance or a biased test set for some reason. By training five (or ten) different models we can understand better what’s going on. Say we trained five models and we use accuracy as our measurement.\n",
    "\n",
    "    - The best scenario is that our accuracy is similar in all our folds, say 92.0, 91.5, 92.0, 92.5 and 91.8. This means that our algorithm (and our data) is consistent and we can be confident that by training it on all the data set and deploy it in production will lead to similar performance.\n",
    "\n",
    "    - However, we could end up in a slightly different scenario, say 92.0, 44.0, 91.5, 92.5 and 91.8. These results look very strange. It looks like one of our folds is from a different distribution, we have to go back and make sure that our data is what we think it is.\n",
    "    \n",
    "   > By using Cross-Validation, we are able to get more metrics and draw important conclusion both about our algorithm and our data.\n",
    "   \n",
    "3. __Use Models Stacking:__ we may create a Random Forest Model that predicts something for us, and right after that, we want to do a Linear Regression that will rely on previous predictions and produce some real number. The critical part here is that our second model must learn on the predictions of our first model. The best solution here is to use two different datasets for each model. We train our Random Forest on dataset A. Then we use dataset B to make a prediction using it.\n",
    "\n",
    "\n",
    "4. __Work with Dependent/Grouped Data__\n",
    "5. __Parameters Fine-Tuning__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Qtns\n",
    "__1. Why the error estimate obtained through k-fold-cross-validation is almost unbiased?__\n",
    "\n",
    "   __Ans:__ We repeat the model evaluation process multiple times (instead of one time) and calculate the mean skill. The mean estimate of any parameter is less biased than a one-shot estimate. There is still some bias though.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
