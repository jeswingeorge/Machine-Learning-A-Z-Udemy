{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which machine algorithms require data scaling/normalization?\n",
    "\n",
    "All algorithms that are distance based require scaling. This includes all curve fitting algorithms (linear/non-linear regressions), logistic regression, KNN, SVM, Neural Networks, clustering algorithms like k-means clustering etc.\n",
    "\n",
    "Algorithms that are used for matrix factorization, decomposition or dimensionality reduction like PCA, SVD, Factorization Machines etc also require normalization.\n",
    "\n",
    "Algorithms that do not require normalization/scaling are the ones that rely on rules. They would not be affected by any monotonic transformations of the variables. Scaling is a monotonic transformation - the relative order of smaller to larger value in a variable is maintained post the scaling. Examples of algorithms in this category are all the tree based algorithms - CART, Random Forests, Gradient Boosted Decision Trees etc. These algorithms utilize rules (series of inequalities) and do not require normalization.\n",
    "\n",
    "Also, Algorithms that rely on distributions of the variables, like Naive Bayes also do not need scaling.\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "1. [All about Feature Scaling](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35)\n",
    "2. [Effect of Feature Standardization on Linear Support Vector Machines](https://towardsdatascience.com/effect-of-feature-standardization-on-linear-support-vector-machines-13213765b812)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How different probability distributions are used on ML?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3. [Why is lasso more robust to outliers compared to ridge?](https://stats.stackexchange.com/questions/404495/why-is-lasso-more-robust-to-outliers-compared-to-ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider what an outlier does to the coefficients:\n",
    "\n",
    "- If it has low leverage, nothing;\n",
    "- If it has high leverage, it pulls the coefficient towards itself (either increasing or decreasing it).\n",
    "\n",
    "When you apply the LASSO penalty to OLS, you penalize the coefficients by summing their absolute values. An outlier with sufficient leverage increases/decreases a coefficient, also affecting the penalty linearly. This will somewhat increase/decrease the penalty to the the other coefficients, but not by much.\n",
    "\n",
    "When you apply the ridge penalty, the sum of squared coefficients shrinks the coefficient. This means that outlyingness will not only increase the OLS quadratically, but also the penalty. As such, all the other coefficients might be shrunk considerably more/less (depending on what kind of outlier you're dealing with).\n",
    "\n",
    "\n",
    "This sensitivity of the penalty to changes in the coefficients (and thus to outliers) means that ridge is less robust to outliers than LASSO.\n",
    "***\n",
    "\n",
    "### 4. Choosing the correct classification algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\choose_clf.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
