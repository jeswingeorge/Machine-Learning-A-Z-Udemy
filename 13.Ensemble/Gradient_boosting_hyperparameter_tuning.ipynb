{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Parameter Tuning in Gradient Boosting (GBM)](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n",
    "\n",
    "# [Gradient Boosting Decision tree](https://towardsdatascience.com/gradient-boosted-decision-trees-explained-9259bd8205af)\n",
    "\n",
    "__Boosting algorithms play a crucial role in dealing with bias variance trade-off.  Unlike bagging algorithms, which only controls for high variance in a model, boosting controls both the aspects (bias & variance), and is considered to be more effective.__\n",
    "\n",
    "\n",
    "### Difference between boosting and random forest\n",
    "\n",
    "![](images\\gbm_5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How Boosting Works ?\n",
    "\n",
    "Boosting is a sequential technique which works on the principle of __ensemble__. It combines a set of __weak learners__ and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. This technique is followed for a classification problem while a similar technique is used for regression.\n",
    "\n",
    "![](images\\gbm1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient boosting algorithm sequentially combines weak learners in way that each new learner fits to the residuals from the previous step so that the model improves. The final model aggregates the results from each step and a strong learner is achieved. \n",
    "\n",
    "__Gradient boosted decision trees__ algorithm uses decision trees as week learners. A loss function is used to detect the residuals. For instance, mean squared error (MSE) can be used for a regression task and logarithmic loss (log loss) can be used for classification tasks. It is worth noting that existing trees in the model do not change when a new tree is added. The added decision tree fits the residuals from the current model. \n",
    "\n",
    "![](images\\gbm_6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GBM Parameters\n",
    "\n",
    "The overall parameters of this ensemble model can be divided into 3 categories:\n",
    "\n",
    "1. __Tree-Specific Parameters:__ These affect each individual tree in the model.\n",
    "2. __Boosting Parameters:__ These affect the boosting operation in the model.\n",
    "3. __Miscellaneous Parameters:__ Other parameters for overall functioning.\n",
    "\n",
    "\n",
    "![](images\\gbm_2.PNG)\n",
    "\n",
    "The parameters used for defining a tree are:\n",
    "\n",
    "1. __min_samples_split__\n",
    "    - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "\n",
    "\n",
    "2. __min_samples_leaf__\n",
    "    - Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "    - Used to control over-fitting similar to min_samples_split.\n",
    "    - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "    \n",
    "    \n",
    "3. __min_weight_fraction_leaf__\n",
    "    - Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "    - Only one of #2 and #3 should be defined.\n",
    "    \n",
    "4. __max_depth__\n",
    "    - The maximum depth of a tree.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - The higher value of maximum depth causes overfitting, and a lower value causes underfitting \n",
    "    \n",
    "    ![](images\\gbm_3.png)\n",
    "    \n",
    "5. __max_leaf_nodes__\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "    \n",
    "6. __max_features__\n",
    "    - The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "    - As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "    - Higher values can lead to over-fitting but depends on case to case.    \n",
    "    \n",
    "Before moving on to other parameters, lets see the overall pseudo-code of the GBM algorithm for 2 classes:\n",
    "\n",
    "![](images\\gbm_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Boosting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A problem with gradient boosted decision trees is that they are quick to learn and overfit training data. One effective way to slow down learning in the gradient boosting model is to use a learning rate, also called shrinkage (or eta in XGBoost \n",
    "\n",
    "#### Learning rate and n_estimators\n",
    "\n",
    "Hyperparemetes are key parts of learning algorithms which effect the performance and accuracy of a model. __Learning rate__ and __n_estimators__ are two critical hyperparameters for gradient boosting decision trees. `Learning rate` $\\alpha$ simply means how fast the model learns. Each tree added modifies the overall model. The magnitude of the modification is controlled by learning rate. \n",
    "\n",
    "The steps of gradient boosted decision tree algorithms with learning rate introduced:\n",
    "\n",
    "![](images\\gbm_7.PNG)\n",
    "\n",
    "\n",
    "__The lower the learning rate, the slower the model learns.__ The __advantage__ of slower learning rate is that the model becomes more robust and generalized. In statistical learning, models that learn slowly perform better. However, learning slowly comes at a cost. It takes more time to train the model which brings us to the other significant hyperparameter.\n",
    "\n",
    "__n_estimator__ is the number of trees used in the model. If the learning rate is low, we need more trees to train the model. However, we need to be very careful at selecting the number of trees. It creates a high risk of overfitting to use too many trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on overfitting\n",
    "\n",
    "__One key difference between random forests and gradient boosting decision trees is the number of trees used in the model. Increasing the number of trees in random forests does not cause overfitting.__ After some point, the accuracy of the model does not increase by adding more trees but it is also not negatively effected by adding excessive trees. You still do not want to add unnecessary amount of trees due to computational reasons but there is no risk of overfitting associated with the number of trees in random forests. \n",
    "\n",
    "\n",
    "However, the number of trees in gradient boosting decision trees is very critical in terms of overfitting. Adding too many trees will cause overfitting so it is important to stop adding trees at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from these, there are certain miscellaneous parameters which affect overall functionality:\n",
    "\n",
    "![](images\\gbm_10.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.model_selection import cross_val_score  #Additional scklearn functions\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "# pandas defaults\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, dtrain[predictors], dtrain['Disbursed'], cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_14.PNG)\n",
    "\n",
    "![](images\\gbm_15.PNG)\n",
    "\n",
    "![](images\\gbm_16.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_17.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_178.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_18.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find that optimum value is 7, which is also the square root. So our initial value was the best. You might be anxious to check for lower values and you should if you like. I’ll stay with 7 for now. With this we have the final tree-parameters as:\n",
    "\n",
    "- min_samples_split: 1200\n",
    "- min_samples_leaf: 60\n",
    "- max_depth: 9\n",
    "- max_features: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning subsample and making models with lower learning rate\n",
    "\n",
    "The next step would be try different subsample values. Lets take values 0.6,0.7,0.75,0.8,0.85,0.9.\n",
    "\n",
    "![](images\\gbm_19.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we found 0.85 as the optimum value. Finally, we have all the parameters needed. Now, we need to lower the learning rate and increase the number of estimators proportionally. Note that these trees might not be the most optimum values but a good benchmark.\n",
    "\n",
    "As trees increase, it will become increasingly computationally expensive to perform CV and find the optimum values. \n",
    "\n",
    "Lets decrease the learning rate to half, i.e. 0.05 with twice (120) the number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_20.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now lets reduce learning rate to one-tenth of the original value(), i.e. 0.01 for 600 trees ---> cv_mean = 0.8409\n",
    "2. Lets decrease to one-twentieth of the original value, i.e. 0.005 for 1200 trees.---> cv_mean = 0.8392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\gbm_8.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
