{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Topics:\n",
    "1. Need for PCA\n",
    "2. What is PCA?\n",
    "3. Step by step computation of PCA\n",
    "4. pca with Python\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Need for PCA\n",
    "\n",
    "![](pca.PNG)\n",
    "\n",
    "High dimension data is extremely complex to process due to inconsistencies in the features which increase the computation time and make data processing and EDA more convoluted.  \n",
    "\n",
    "High dimensional data can be easily found in cases such as image processing, NLP, image translation and so on. So to get rid of this curse we came up with a process called __dimensionality reduction.__ Now dimensionality reduction techniques can be used to filter only a limited number of significant features which are needed to train your ML model. This is where PCA comes into the picture. PCA helps in implementing ML easily.\n",
    "\n",
    "## 2. What is PCA?\n",
    "\n",
    "__Principal Component Analysis (PCA)__ is a dimensionality reduction technique that enables you to identify correlations and patterns in a data set so that it can be transformed into a dataset of significantly lower dimension without loss of any important information. \n",
    "\n",
    "![](pca2.PNG)\n",
    "\n",
    "When performing dimensionality reduction using PCA or any other method, it should be performed in such a way that significant data is retained in the new dataset. Basically we are narrowing down a couple of variables from your orignal dataset to your final dataset which has all important information.\n",
    "\n",
    "\n",
    "## 3. Step by step computation of PCA\n",
    "\n",
    "![](pca3.PNG)\n",
    "\n",
    "#### Step 1: Standardization of the data\n",
    "\n",
    "Standardization is all about scaling your data in such a way that all the variables and their values lie within a similar range.\n",
    "\n",
    "Missing out on standardization will result in a biased outcome i.e., output will be more impacted by the larger of the two values (eg: salary and age) so the output will not be accurate.\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "where,  \n",
    "$\\mu$ - mean  \n",
    "$\\sigma$ - standard deviation  \n",
    "x - Variable value  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the covariance matrix\n",
    "\n",
    "A covariance matrix (its a square matrix) expresses the correlation between the different variables in the dataset. It is essential to identify heavily dependant variables because they contain biased and redundant information which reduces the overall performance of the model.\n",
    "\n",
    "![](pca4.PNG)\n",
    "\n",
    "#### 3. Calculating the eigen values and eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
