{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "\n",
    "__Feature selection__ is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable. These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and output variables.\n",
    "\n",
    "Will learn here:\n",
    "1. There are two main types of feature selection techniques: supervised and unsupervised, and supervised methods may be divided into wrapper, filter and intrinsic.\n",
    "2. Filter-based feature selection methods use statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features.\n",
    "3. Statistical measures for feature selection must be carefully chosen based on the data type of the input variable and the output or response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is divided into 4 parts; they are:\n",
    "\n",
    "1. Feature Selection Methods\n",
    "2. Statistics for Filter Feature Selection Methods\n",
    "    - Numerical Input, Numerical Output\n",
    "    - Numerical Input, Categorical Output\n",
    "    - Categorical Input, Numerical Output\n",
    "    - Categorical Input, Categorical Output\n",
    "3. Tips and Tricks for Feature Selection\n",
    "    - Correlation Statistics\n",
    "    - Selection Method\n",
    "    - Transform Variables\n",
    "    - What Is the Best Method?\n",
    "4. Worked Examples\n",
    "    - Regression Feature Selection\n",
    "    - Classification Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection Methods\n",
    "\n",
    "__Feature selection__ methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable.\n",
    "\n",
    "Some predictive modeling problems have a large number of variables that can slow the development and training of models and require a large amount of system memory. Additionally, the performance of some models can degrade when including input variables that are not relevant to the target variable.\n",
    "\n",
    "One way to think about feature selection methods are in terms of supervised and unsupervised methods.\n",
    "> The difference has to do with whether features are selected based on the target variable or not. Unsupervised feature selection techniques ignores the target variable, such as methods that remove redundant variables using correlation. Supervised feature selection techniques use the target variable, such as methods that remove irrelevant variables..\n",
    "\n",
    "Another way to consider the mechanism used to select features which may be divided into __wrapper__ and __filter__ methods. These methods are almost always supervised and are evaluated based on the performance of a resulting model on a hold out dataset.\n",
    "\n",
    "__Wrapper feature selection methods__ create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. These methods are unconcerned with the variable types, although they can be computationally expensive. \n",
    "> RFE is a good example of a wrapper feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\n",
    "\n",
    "> Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion.\n",
    "\n",
    "Finally, there are some machine learning algorithms that perform feature selection automatically as part of learning the model. We might refer to these techniques as __intrinsic feature selection methods__. The model will only include predictors that help maximize accuracy. In these cases, the model can pick and choose which representation of the data is best.\n",
    "\n",
    "This includes algorithms such as penalized regression models like Lasso and decision trees, including ensembles of decision trees like random forest.\n",
    "\n",
    "\n",
    "#### Difference between Feature selection and dimensionality reduction\n",
    "\n",
    "Feature selection is also related to dimensionally reduction techniques in that both methods seek fewer input variables to a predictive model. The difference is that __feature selection__ select features to keep or remove from the dataset, whereas __dimensionality reduction__ create a projection of the data resulting in entirely new input features. As such, dimensionality reduction is an alternate to feature selection rather than a type of feature selection.\n",
    "\n",
    "\n",
    "## We can summarize feature selection as follows.\n",
    "\n",
    "- Feature Selection: Select a subset of input features from the dataset.\n",
    "    - Unsupervised: Do not use the target variable (e.g. remove redundant variables).\n",
    "         - Correlation\n",
    "    - Supervised: Use the target variable (e.g. remove irrelevant variables).\n",
    "        - __Wrapper__: Search for well-performing subsets of features.\n",
    "            - RFE\n",
    "        - __Filter__: Select subsets of features based on their relationship with the target.\n",
    "             - Statistical Methods\n",
    "             - Feature Importance Methods\n",
    "        - __Intrinsic__: Algorithms that perform automatic feature selection during training.\n",
    "             - Decision Trees, Ridge/Lasso/Elastic-net regression.\n",
    "\n",
    "- Dimensionality Reduction: Project input data into a lower-dimensional feature space. Eg: PCA, SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fs1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics for Filter-Based Feature Selection Methods\n",
    "\n",
    "It is common to use correlation type statistical measures between input and output variables as the basis for filter feature selection.\n",
    "\n",
    "As such, the choice of statistical measures is highly dependent upon the variable data types.\n",
    "\n",
    "Common data types include numerical (such as height) and categorical (such as a label), although each may be further subdivided such as integer and floating point for numerical variables, and boolean, ordinal, or nominal for categorical variables.\n",
    "\n",
    "Common input variable data types:\n",
    "\n",
    "1. Numerical Variables\n",
    "    - Integer Variables.\n",
    "    - Floating Point Variables.\n",
    "2. Categorical Variables.\n",
    "    - Boolean Variables (dichotomous).\n",
    "    - Ordinal Variables.\n",
    "    - Nominal Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fs2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical measures used in filter-based feature selection are generally calculated one input variable at a time with the target variable. As such, they are referred to as univariate statistical measures. This may mean that any interaction between input variables is not considered in the filtering process.\n",
    "\n",
    "> Most of these techniques are univariate, meaning that they evaluate each predictor in isolation. In this case, the existence of correlated predictors makes it possible to select important, but redundant, predictors. The obvious consequences of this issue are that too many predictors are chosen and, as a result, collinearity problems arise.\n",
    "\n",
    "With this framework, let’s review some univariate statistical measures that can be used for filter-based feature selection.\n",
    "\n",
    "![](fs3.PNG)\n",
    "\n",
    "#### Numerical Input, Numerical Output\n",
    "This is a regression predictive modeling problem with numerical input variables.\n",
    "\n",
    "The most common techniques are to use a correlation coefficient, such as Pearson’s for a linear correlation, or rank-based methods for a nonlinear correlation.\n",
    "\n",
    "- Pearson’s correlation coefficient (linear).\n",
    "- Spearman’s rank coefficient (nonlinear)\n",
    "\n",
    "#### Numerical Input, Categorical Output\n",
    "This is a classification predictive modeling problem with numerical input variables.\n",
    "\n",
    "This might be the most common example of a classification problem,\n",
    "\n",
    "Again, the most common techniques are correlation based, although in this case, they must take the categorical target into account.\n",
    "\n",
    "- ANOVA correlation coefficient (linear).\n",
    "- Kendall’s rank coefficient (nonlinear).\n",
    "Kendall does assume that the categorical variable is ordinal.\n",
    "\n",
    "#### Categorical Input, Numerical Output\n",
    "This is a regression predictive modeling problem with categorical input variables.\n",
    "\n",
    "This is a strange example of a regression problem (e.g. you would not encounter it often).\n",
    "\n",
    "Nevertheless, you can use the same “Numerical Input, Categorical Output” methods (described above), but in reverse.\n",
    "\n",
    "#### Categorical Input, Categorical Output\n",
    "This is a classification predictive modeling problem with categorical input variables.\n",
    "\n",
    "The most common correlation measure for categorical data is the chi-squared test. You can also use mutual information (information gain) from the field of information theory.\n",
    "\n",
    "- Chi-Squared test (contingency tables).\n",
    "- Mutual Information.\n",
    "\n",
    "In fact, mutual information is a powerful method that may prove useful for both categorical and numerical data, e.g. it is agnostic to the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tips and Tricks for Feature Selection\n",
    "This section provides some additional considerations when using filter-based feature selection.\n",
    "\n",
    "### Correlation Statistics\n",
    "The scikit-learn library provides an implementation of most of the useful statistical measures.\n",
    "\n",
    "For example:\n",
    "- __Pearson’s Correlation Coefficient__: [f_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "- __ANOVA__: [f_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "- __Chi-Squared__: [chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "- __Mutual Information__: [mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and [mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "\n",
    "Also, the SciPy library provides an implementation of many more statistics, such as Kendall’s tau ([kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)) and Spearman’s rank correlation ([spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)).\n",
    "\n",
    "### Selection Method\n",
    "The scikit-learn library also provides many different filtering methods once statistics have been calculated for each input variable with the target.\n",
    "\n",
    "Two of the more popular methods include:\n",
    "\n",
    "- Select the top k variables: [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "- Select the top percentile variables: [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)\n",
    "\n",
    "### Transform Variables\n",
    "Consider transforming the variables in order to access different statistical methods.\n",
    "\n",
    "For example, you can transform a categorical variable to ordinal, even if it is not, and see if any interesting results come out.\n",
    "\n",
    "You can also make a numerical variable discrete (e.g. bins); try categorical-based measures.\n",
    "\n",
    "Some statistical measures assume properties of the variables, such as Pearson’s that assumes a Gaussian probability distribution to the observations and a linear relationship. You can transform the data to meet the expectations of the test and try the test regardless of the expectations and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is the Best Method?\n",
    "There is no best feature selection method.\n",
    "\n",
    "Just like there is no best set of input variables or best machine learning algorithm. At least not universally.\n",
    "\n",
    "Instead, you must discover what works best for your specific problem using careful systematic experimentation.\n",
    "\n",
    "Try a range of different models fit on different subsets of features chosen via different statistical measures and discover what works best for your specific problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
