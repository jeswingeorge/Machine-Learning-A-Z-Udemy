{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R squared Intuition\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination.\n",
    "\n",
    "Before you look at the statistical measures for goodness-of-fit, you should [check the residual plots](https://blog.minitab.com/blog/adventures-in-statistics-2/why-you-need-to-check-your-residual-plots-for-regression-analysis). Residual plots can reveal unwanted residual patterns that indicate biased results more effectively than numbers. When your residual plots pass muster, you can trust your numerical results and check the goodness-of-fit statistics.\n",
    "\n",
    "![](rsq.png)\n",
    "\n",
    "> R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "## Key Limitations of R-squared\n",
    "- R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.\n",
    "- R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!\n",
    "- [The R-squared in your output is a biased estimate of the population R-squared.](https://blog.minitab.com/blog/adventures-in-statistics-2/r-squared-shrinkage-and-power-and-sample-size-guidelines-for-regression-analysis)\n",
    "\n",
    "#### Reference: [Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "# Adjusted $R^2$\n",
    "\n",
    "\n",
    "### Some Problems with R-squared\n",
    "\n",
    "R-squared has additional problems that the adjusted R-squared and predicted R-squared are designed to address.\n",
    "\n",
    "__Problem 1:__ Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.\n",
    "\n",
    "__Problem 2:__ If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and it produces misleadingly high R-squared values and a lessened ability to make predictions.\n",
    "\n",
    "\n",
    "For a linear regression model every additional predictor variable tends to minimize the error of the model. As a result the $R^2$ value will never decreases for any additional number of predictor variables being included in model.\n",
    "\n",
    "\n",
    "__Adjusted $R^2$__ takes into account the number of predictor variables included in the regression model. Unlike $R^2$, adjusted $R^2$ can decrease with increase in  number of predictors.\n",
    "\n",
    "$$ R_a^2 = 1 - \\frac{\\frac{SSE}{n-k-1}}{\\frac{SST}{n-1}} $$\n",
    "where,  \n",
    "n - number of observations  \n",
    "k - number of predictor variables in method\n",
    "\n",
    "> __SST = SSR + SSE__.\n",
    "\n",
    "> $ \\sum(y_{i} - \\bar{y})^{2} = \\sum(\\widehat{y_{i}} - \\bar{y})^{2} + \\sum(y_{i} - \\widehat{y_{i}})^{2}$\n",
    "\n",
    "In addition to $R^2$ and adjusted $R^2$ values, __F-test__ can also be used to suggest that model is significant\n",
    "\n",
    "It can also be said that adj $R^2$ has a penalization factor __(n-k-1)__ so as we add more regresssors the adj $R^2$ decreases. \n",
    "\n",
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but itâ€™s usually not.  It is always lower than the R-squared.\n",
    "\n",
    "In the simplified Best Subsets Regression output below, you can see where the adjusted R-squared peaks, and then declines. Meanwhile, the R-squared continues to increase.\n",
    "\n",
    "![](jku.PNG)\n",
    "\n",
    "__Reference__: [Multiple Regression Analysis: Use Adjusted R-Squared and Predicted R-Squared to Include the Correct Number of Variables](https://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. What are the pros and cons of each model ?\n",
    "\n",
    "Please find here a cheat-sheet that gives you all the pros and the cons of each regression model.\n",
    "\n",
    "2. How do I know which model to choose for my problem ?\n",
    "\n",
    "First, you need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then: If your problem is linear, you should go for Simple Linear Regression if you only have one feature, and Multiple Linear Regression if you have several features.\n",
    "\n",
    "If your problem is non linear, you should go for Polynomial Regression, SVR, Decision Tree or Random Forest. Then which one should you choose among these four ? That you will learn in Part 10 - Model Selection. The method consists of using a very relevant technique that evaluates your models performance, called k-Fold Cross Validation, and then picking the model that shows the best results. Feel free to jump directly to Part 10 if you already want to learn how to do that.\n",
    "\n",
    "3. How can I improve each of these models ?\n",
    "\n",
    "In Part 10 - Model Selection, you will find the second section dedicated to Parameter Tuning, that will allow you to improve the performance of your models, by tuning them. You probably already noticed that each model is composed of two types of parameters: \n",
    "the parameters that are learnt, for example the coefficients in Linear Regression, the hyperparameters.\n",
    "\n",
    "The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value of these hyperparameters, and we haven't searched for their optimal value so that your model reaches even higher performance. Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
