{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R squared \n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination.\n",
    "\n",
    "Before you look at the statistical measures for goodness-of-fit, you should [check the residual plots](https://blog.minitab.com/blog/adventures-in-statistics-2/why-you-need-to-check-your-residual-plots-for-regression-analysis). Residual plots can reveal unwanted residual patterns that indicate biased results more effectively than numbers. When your residual plots pass muster, you can trust your numerical results and check the goodness-of-fit statistics.\n",
    "\n",
    "# Coefficient of determination -  $R^2 = \\frac{SSR}{SST}$\n",
    "\n",
    "The usefulness of a linear regression model for a data set with an outcome variable Y and a predictor variable X, having a linear relationship modeled using simple linear regression can be determined using the __coefficient of determination ($R^2$)__.\n",
    "\n",
    "__SSR - Sum of squared regressions__ - Sum of squared Deviations of predicted values $\\widehat{y_{i}}$ from mean $\\bar{y}$\n",
    "\n",
    "> $SSR = \\sum(\\widehat{y_{i}} - \\bar{y})^{2}$\n",
    "\n",
    "__SST - Sum of squared total__ - total sum of squared deviations of actual values $y_{i}$ from its mean $\\bar{y}$\n",
    "\n",
    "> $SST = \\sum(y_{i}- \\bar{y})^{2}$\n",
    "\n",
    "SST can be thought of as the error in predicting Y without the knowledge of X.\n",
    "\n",
    "The sum of squared total (SST) can also be expressed in terms of sum of squared regression (SSR) and sum of squared errors (SSE) as,\n",
    "> __SST = SSR + SSE__.\n",
    "\n",
    "> $ \\sum(y_{i} - \\bar{y})^{2} = \\sum(\\widehat{y_{i}} - \\bar{y})^{2} + \\sum(y_{i} - \\widehat{y_{i}})^{2}$\n",
    "\n",
    "> R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "## Key Limitations of R-squared\n",
    "- R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.\n",
    "- R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!\n",
    "- [The R-squared in your output is a biased estimate of the population R-squared.](https://blog.minitab.com/blog/adventures-in-statistics-2/r-squared-shrinkage-and-power-and-sample-size-guidelines-for-regression-analysis)\n",
    "\n",
    "#### Reference: [Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "# Adjusted $R^2$\n",
    "\n",
    "\n",
    "### Some Problems with R-squared\n",
    "\n",
    "R-squared has additional problems that the adjusted R-squared and predicted R-squared are designed to address.\n",
    "\n",
    "__Problem 1:__ Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.\n",
    "\n",
    "__Problem 2:__ If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and it produces misleadingly high R-squared values and a lessened ability to make predictions.\n",
    "\n",
    "\n",
    "For a linear regression model every additional predictor variable tends to minimize the error of the model. As a result the $R^2$ value will never decreases for any additional number of predictor variables being included in model.\n",
    "\n",
    "\n",
    "__Adjusted $R^2$__ takes into account the number of predictor variables included in the regression model. Unlike $R^2$, adjusted $R^2$ can decrease with increase in  number of predictors.\n",
    "\n",
    "$$ R_a^2 = 1 - \\frac{\\frac{SSE}{n-k-1}}{\\frac{SST}{n-1}} $$\n",
    "where,  \n",
    "n - number of observations  \n",
    "k - number of predictor variables in method\n",
    "\n",
    "> __SST = SSR + SSE__.\n",
    "\n",
    "> $ \\sum(y_{i} - \\bar{y})^{2} = \\sum(\\widehat{y_{i}} - \\bar{y})^{2} + \\sum(y_{i} - \\widehat{y_{i}})^{2}$\n",
    "\n",
    "In addition to $R^2$ and adjusted $R^2$ values, __F-test__ can also be used to suggest that model is significant\n",
    "\n",
    "It can also be said that adj $R^2$ has a penalization factor __(n-k-1)__ so as we add more regresssors the adj $R^2$ decreases. \n",
    "\n",
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.  It is always lower than the R-squared.\n",
    "\n",
    "In the simplified Best Subsets Regression output below, you can see where the adjusted R-squared peaks, and then declines. Meanwhile, the R-squared continues to increase.\n",
    "\n",
    "![](jku.PNG)\n",
    "\n",
    "__Reference__: [Multiple Regression Analysis: Use Adjusted R-Squared and Predicted R-Squared to Include the Correct Number of Variables](https://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables)\n",
    "\n",
    "\n",
    "***\n",
    "__Reference__: [Tutorial: Understanding Regression Error Metrics in Python](dataquest.io/blog/understanding-regression-error-metrics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set(rc={'figure.figsize':(10,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Units</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Units  Minutes\n",
       "0      1       23\n",
       "1      2       29\n",
       "2      3       49\n",
       "3      4       64\n",
       "4      4       74"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('computers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()  # Ordinary least squares Linear Regression BY DEFAULT\n",
    "model.fit(X = df.loc[:,[\"Units\"]], y= df.loc[:,[\"Minutes\"]])  # pass x and y as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.67042607],\n",
       "       [ 35.17919799],\n",
       "       [ 50.68796992],\n",
       "       [ 66.19674185],\n",
       "       [ 66.19674185],\n",
       "       [ 81.70551378],\n",
       "       [ 97.21428571],\n",
       "       [ 97.21428571],\n",
       "       [112.72305764],\n",
       "       [128.23182957],\n",
       "       [143.7406015 ],\n",
       "       [143.7406015 ],\n",
       "       [159.24937343],\n",
       "       [159.24937343]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(df.loc[:,[\"Units\"]])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "import sklearn.metrics as metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred) # SSR\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute error\n",
    "\n",
    "The __mean absolute error (MAE)__ is the simplest regression error metric to understand. We’ll calculate the residual for every data point, taking only the absolute value of each so that negative and positive residuals do not cancel out. We then take the average of all these residuals. Effectively, MAE describes the typical magnitude of the residuals. \n",
    "\n",
    "![](mae.PNG)\n",
    "\n",
    "The picture below is a graphical description of the MAE. The green line represents our model’s predictions, and the blue points represent our data.\n",
    "\n",
    "![](mae2.PNG)\n",
    "\n",
    "Because we use the absolute value of the residual, the MAE does not indicate underperformance or overperformance of the model (whether or not the model under or overshoots actual data). Each residual contributes proportionally to the total amount of error, meaning that larger errors will contribute linearly to the overall error. Like we’ve said above, a small MAE suggests the model is great at prediction, while a large MAE suggests that your model may have trouble in certain areas. A MAE of 0 means that your model is a perfect predictor of the outputs (but this will almost never happen).\n",
    "\n",
    "While the MAE is easily interpretable, using the absolute value of the residual often is not as desirable as squaring this difference. Depending on how you want your model to treat outliers, or extreme values, in your data, you may want to bring more attention to these outliers or downplay them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242391693519507\n"
     ]
    }
   ],
   "source": [
    "# calculating mean absolute error\n",
    "mae_sum = 0\n",
    "for minutes, x in zip(df.loc[:,\"Minutes\"], df.loc[:,\"Units\"]):\n",
    "    prediction = model.predict([[x]])\n",
    "    mae_sum += abs(minutes - prediction)\n",
    "mae = mae_sum / len(df.loc[:,\"Minutes\"])\n",
    "print(mae[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using in-built function: `metrics.mean_absolute_error()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.242391693519507"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_absolute_error(df.loc[:,\"Minutes\"], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean square error\n",
    "\n",
    "The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. We can see this difference in the equation below.\n",
    "\n",
    "![](mse.PNG)\n",
    "\n",
    "### Consequences of the Square Term\n",
    "Because we are squaring the difference, the MSE will almost always be bigger than the MAE. For this reason, we cannot directly compare the MAE to the MSE. We can only compare our model’s error metrics to those of a competing model. The effect of the square term in the MSE equation is most apparent with the presence of outliers in our data. While each residual in MAE contributes proportionally to the total error, the error grows quadratically in MSE. This ultimately means that outliers in our data will contribute to much higher total error in the MSE than they would the MAE. Similarly, our model will be penalized more for making predictions that differ greatly from the corresponding actual value. This is to say that large differences between actual and predicted are punished more in MSE than in MAE. The following picture graphically demonstrates what an individual residual in the MSE might look like.\n",
    "\n",
    "![](mse2.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.91774078]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_sum = 0\n",
    "for minutes, x in zip(df.loc[:,\"Minutes\"], df.loc[:,\"Units\"]):\n",
    "    prediction = model.predict([[x]])\n",
    "    mse_sum += (minutes - prediction)**2\n",
    "mse = mse_sum / len(df.loc[:,\"Minutes\"])\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.917740780522703"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using the library\n",
    "metrics.mean_squared_error(df.loc[:,\"Minutes\"], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of outliers\n",
    "\n",
    "Do we include the outliers in our model creation or do we ignore them? The answer to this question is dependent on the field of study, the data set on hand and the consequences of having errors in the first place. For example, I know that some video games achieve superstar status and thus have disproportionately higher earnings. Therefore, it would be foolish of me to ignore these outlier games because they represent a real phenomenon within the data set. I would want to use the MSE to ensure that my model takes these outliers into account more.\n",
    "\n",
    "__If I wanted to downplay their significance, I would use the MAE since the outlier residuals won’t contribute as much to the total error as MSE.__ Ultimately, the choice between is MSE and MAE is application-specific and depends on how you want to treat large errors. Both are still viable error metrics, but will describe different nuances about the prediction errors of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# root mean squared error (RMSE)\n",
    "\n",
    "Since the MSE and RMSE both square the residual, they are similarly affected by outliers. __The RMSE is analogous to the standard deviation (MSE to variance) and is a measure of how large your residuals are spread out.__\n",
    "\n",
    "## $$RMSE = (\\frac{1}{n}\\sum(y - \\hat{y})^2)^\\frac{1}{2} $$\n",
    "\n",
    "When standardized observations and forecasts are used as RMSE inputs, there is a direct relationship with the correlation coefficient. For example, if the correlation coefficient is 1, the RMSE will be 0, because all of the points lie on the regression line (and therefore there are no errors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9917673]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_sum = 0\n",
    "for minutes, x in zip(df.loc[:,\"Minutes\"], df.loc[:,\"Units\"]):\n",
    "    prediction = model.predict([[x]])\n",
    "    mse_sum += (minutes - prediction)**2\n",
    "mse = mse_sum / len(df.loc[:,\"Minutes\"])\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.991767300317865"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(metrics.mean_squared_error(df.loc[:,\"Minutes\"], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute percentage error\n",
    "\n",
    "The mean absolute percentage error (MAPE) is the percentage equivalent of MAE. The equation looks just like that of MAE, but with adjustments to convert everything into percentages.\n",
    "\n",
    "![](mape.PNG)\n",
    "\n",
    "Just as MAE is the average magnitude of error produced by your model, the MAPE is how far the model’s predictions are off from their corresponding outputs on average. Like MAE, MAPE also has a clear interpretation since percentages are easier for people to conceptualize. __Both MAPE and MAE are robust to the effects of outliers thanks to the use of absolute value.__\n",
    "\n",
    "![](mape2.PNG)\n",
    "\n",
    "However for all of its advantages, we are more limited in using MAPE than we are MAE. Many of MAPE’s weaknesses actually stem from use division operation. Now that we have to scale everything by the actual value, MAPE is undefined for data points where the value is 0. Similarly, the MAPE can grow unexpectedly large if the actual values are exceptionally small themselves. Finally, the MAPE is biased towards predictions that are systematically less than the actual values themselves. That is to say, MAPE will be lower when the prediction is lower than the actual compared to a prediction that is higher by the same amount. The quick calculation below demonstrates this point.\n",
    "\n",
    "![](mape3.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.98748224]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_sum = 0\n",
    "for minutes, x in zip(df.loc[:,\"Minutes\"], df.loc[:,\"Units\"]):\n",
    "    prediction = model.predict([[x]])\n",
    "    mape_sum += (abs((minutes - prediction))/minutes)\n",
    "mape = mape_sum/len(df.loc[:,\"Minutes\"])\n",
    "mape*100 # %ge conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know for sure that there are no data points for which there are zero sales, so we are safe to use MAPE.\n",
    "\n",
    "Remember that we must interpret it in terms of percentage points. MAPE states that our model’s predictions are, on average, 5.98% off from actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean percentage error\n",
    "\n",
    "The mean percentage error (MPE) equation is exactly like that of MAPE. The only difference is that it lacks the absolute value operation.\n",
    "\n",
    "![](mpe.PNG)\n",
    "\n",
    "\n",
    "Even though the MPE lacks the absolute value operation, it is actually its absence that makes MPE useful. Since positive and negative errors will cancel out, we cannot make any statements about how well the model predictions perform overall. However, if there are more negative or positive errors, this bias will show up in the MPE. Unlike MAE and MAPE, MPE is useful to us because it allows us to see if our model systematically __underestimates (more negative error)__ or __overestimates (positive error)__.\n",
    "\n",
    "![](mpe2.PNG)\n",
    "\n",
    "If you’re going to use a relative measure of error like MAPE or MPE rather than an absolute measure of error like MAE or MSE, you’ll most likely use MAPE. MAPE has the advantage of being easily interpretable, but you must be wary of data that will work against the calculation (i.e. zeroes). You can’t use MPE in the same way as MAPE, but it can tell you about systematic errors that your model makes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00334345]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpe_sum = 0\n",
    "for minutes, x in zip(df.loc[:,\"Minutes\"], df.loc[:,\"Units\"]):\n",
    "    prediction = model.predict([[x]])\n",
    "    mpe_sum += ((minutes - prediction)/minutes)\n",
    "mpe = mpe_sum/len(df.loc[:,\"Minutes\"])\n",
    "mpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the MPE indicates to us that it actually systematically underestimates the minutes. Knowing this aspect about our model is helpful to us since it allows us to look back at the data and reiterate on which inputs to include that may improve our metrics. \n",
    "\n",
    "![](conclusion.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
