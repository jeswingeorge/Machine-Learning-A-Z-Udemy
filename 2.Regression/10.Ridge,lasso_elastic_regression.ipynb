{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression\n",
    "\n",
    "Here \n",
    "\n",
    "![](images/10_1.PNG)\n",
    "\n",
    "***\n",
    "\n",
    "We try to create a linear regression model or try to fit a linear model on a given dataset of weight vs sizes.\n",
    "\n",
    "\n",
    "![](images/10_b.PNG)\n",
    "\n",
    "![](images/10_c.png)\n",
    "\n",
    "When we have a lot of measurements we can be fairly confident that the Least squares line accurately reflects the relationship between Size and weight.\n",
    "\n",
    "But what if the dataset has very few points.\n",
    "\n",
    "![](images/10_d.PNG)\n",
    "\n",
    "\n",
    "The sum of the squared residuals for just the __Two Red Points__, the __Training Data__ is small(in this case it is 0) but the sum of the squared residuals for the __Green Points__, the Testing Data is large and this means that the fitted line has high variance.\n",
    "\n",
    "> In ML lingo, we can say that the fitted line is overfitted to the training data.\n",
    "\n",
    "![](images/10_e.PNG)\n",
    "\n",
    "![](images/10_f.PNG)\n",
    "\n",
    "![](images/10_g.PNG)\n",
    "\n",
    "In other words, by starting with a slightly wrong fit, __Ridge Regression__ can provide better long term predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Ridge regression in detail\n",
    "\n",
    "![](images/10_h.PNG)\n",
    "\n",
    "\n",
    "![](images/10_i.PNG)\n",
    "\n",
    "$\\lambda*slope^{2}$ adds a penalty to the traditional __least squares__ method and $\\lambda$ determines how severe that penalty is.\n",
    "\n",
    "![](images/10_j.PNG)\n",
    "\n",
    "![](images/10_k.PNG)\n",
    "\n",
    "Without the small amount of __Bias__ that the penalty creates, the __Least squares fit__ has a large amount of variance. In contrast, the Ridge regression line which has the small amount of __Bias__ due to the penalty has less __Variance__. \n",
    "\n",
    "## Effect of the ridge regression penalty on the fitting line\n",
    "\n",
    "\n",
    "#### Case 1: If the fitting lines slope=1\n",
    "\n",
    "![](images/10_l.PNG)\n",
    "\n",
    "#### Case 2: If the slope is steep\n",
    "\n",
    "![](images/10_m.PNG)\n",
    "\n",
    "#### Case 3: If the slope is small\n",
    "\n",
    "![](images/10_n.PNG)\n",
    "\n",
    "Now lets go back to the __Least Squares__ and __Ridge regression__ lines fit to the two data points.\n",
    "\n",
    "![](images/10_o.PNG)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Lambda $\\lambda$\n",
    "\n",
    "- $\\lambda$ can take any value from 0 to +ve infinity [0,$\\propto$).\n",
    "- when $\\lambda = 0$ then Ridge regression will only minimise the SSR as the penalty parameter will be zero and ridge regression line will be the same as the least squares fit line.\n",
    "- As we increase $\\lambda$ the slope will go on decreasing and the larger we make lambda the slope get aymptotically close to __0__. So the larger lambda gets our prediction for size becomes less and less sensitive to weight (i.e., x-value).\n",
    "\n",
    "#### So how do we decide to take which value of $\\lambda$?\n",
    "\n",
    "We just try a bunch of values for $\\lambda$ and use __cross validation__, typically __10-fold cross validation__ to determine which one results in the lowest __variance__.\n",
    "\n",
    "> $lambda$ is determined using cross-validation. \n",
    "\n",
    "***\n",
    "\n",
    "## Ridge regression with discrete variables\n",
    "\n",
    "Ridge regression also works when we use a discrete variable like __Normal diet vs high fat diet__ to predict __size__.\n",
    "\n",
    "![](images/10_p.png)\n",
    "\n",
    "Lets call that distance `Diet Distance`.\n",
    "\n",
    "From eqn:  `Size = 1.5 + (0.7 x High_Fat_Diet)`\n",
    "\n",
    "`High_Fat_Diet` = 0 for mice on Normal diet  \n",
    "Or, `High_Fat_Diet` = 1 for mice on High Fat Diet  \n",
    "\n",
    "![](images/10_q.PNG)\n",
    "\n",
    "![](images/10_r.PNG)\n",
    "\n",
    "![](images/10_s.PNG)\n",
    "\n",
    "![](images/10_t.PNG)\n",
    "\n",
    "![](images/10_u.PNG)\n",
    "\n",
    "![](images/10_v.PNG)\n",
    "\n",
    "When $\\lambda = 0$ the whole term `$\\lambda*Diet-Differecne^2$` becomes zero and we get the same least squares equation. \n",
    "\n",
    "But when $\\lambda$ gets large the only way to minimize the whole equation is to shrink the Diet-Distance down.\n",
    "\n",
    "![](images/10_w.PNG)\n",
    "\n",
    "![](images/10_x.PNG)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T10:20:07.523364Z",
     "start_time": "2020-04-14T10:20:07.507370Z"
    }
   },
   "source": [
    "# Ridge regression on Logistic regression\n",
    "\n",
    "![](images/10_y.PNG)\n",
    "\n",
    "Equation for Logistic Regression:\n",
    "\n",
    "$$ Obese = c + slope * weight$$  where c:  y-intercept\n",
    "\n",
    "Ridge regression would shrink the estimate for the slope, making our prediction about whether or not a mouse is obese less sensitive to weight.\n",
    "\n",
    "![](images/10_z.PNG)\n",
    "\n",
    "***\n",
    "\n",
    "So far we've seen simple examples of how __Ridge regression__ helps __reduce Variance__ by __shrinking parameters__ and __making our predictors less sensitive to them__.\n",
    "\n",
    "But we can apply ridge regression to complicated models as well.\n",
    "\n",
    "![](images/10_aa.PNG)\n",
    "\n",
    "![](images/10_ab.PNG)\n",
    "\n",
    "Now the Ridge regression Penalty contains the parameters for the slope and the difference between diets.\n",
    "\n",
    "In general. the ridge regression penalty conatins all of the parameters except for the __y-intercept__.\n",
    "\n",
    "![](images/10_ac.PNG)\n",
    "\n",
    "For estimating 2 parameters (i.e, to a fit a simple linear regression) we need minimum two points (2d). Similarly for estimating 3 parameters we need 3 points as we need a hyperplane here (3D).\n",
    "So for estimating n-parameters we need n-data points.\n",
    "\n",
    "But sometimes in practice its hard to get huge data points say(we need to collect gene exprssion measurements from 10,001 mice which is crazy expensive and time consuming. In practice, a huge dataset might have measurements from 500 mice). __So what do we do if we have an equation with 10,001 parameters and only 500 data points??__\n",
    "\n",
    "> We use Ridge regression.\n",
    "\n",
    "![](images/10_ad.PNG)\n",
    "\n",
    "![](images/10_ae.PNG)\n",
    "\n",
    "***\n",
    "\n",
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation that Ridge regression tends to minimise:\n",
    "\n",
    "> Sum of squared residuals + $\\lambda * (slope)^2$\n",
    "\n",
    "### Equation that Ridge regression tends to minimise:\n",
    "\n",
    "> Sum of squared residuals + $\\lambda *$|slope|\n",
    "\n",
    "![](images/10_af.PNG)\n",
    "\n",
    "![](images/10_ag.PNG)\n",
    "\n",
    "![](images/10_ah.PNG)\n",
    "\n",
    "![](images/10_ai.PNG)\n",
    "\n",
    "![](images/10_agk.PNG)\n",
    "\n",
    "![](images/10_fdg.PNG)\n",
    "\n",
    "> Since __Lasso Regression__ can exclude useless variables from equations, it is a little better than __Ridge regression__ at reducing the variance in models that contain a lot of useless variables. So final equation of Lasso regression is funier and easier to interpret.\n",
    "\n",
    "> But ridge regression tends to work better when most variables are useful.\n",
    "\n",
    "\n",
    "### But what do we do when we have many more variables? So we need not choose between Lasso and ridge regression and can use the Elastic-net regression instead. \n",
    "\n",
    "![](images/10_bam.PNG)\n",
    "\n",
    "![](images/10_bmb2.PNG)\n",
    "\n",
    "![](images/11a.PNG)\n",
    "\n",
    "> when $\\lambda_1$ and $\\lambda_2$ equal to zero, then we get least squares fit.\n",
    "\n",
    "> when $\\lambda_1=0$ and $\\lambda_2>0$ equal to zero, then we get Lasso regression.\n",
    "\n",
    "> when $\\lambda_1>0$ and $\\lambda_2=0$ equal to zero, then we get Ridge regression.\n",
    "\n",
    "> when $\\lambda_1>0$ and $\\lambda_2>0$ equal to zero, then we get Elastic-net regression.\n",
    "\n",
    "![](images/11b.PNG)\n",
    "\n",
    "![](images/11c.PNG)\n",
    "\n",
    "![](images/11d.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
