{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Confusion Matrix and why you need it?\n",
    "\n",
    "A __confusion matrix__ is a table that is often used to describe the __performance of a classification model (or \"classifier\")__ on a set of test data for which the true values are known. \n",
    "\n",
    "It is a table with 4 different combinations of predicted and actual values.\n",
    "\n",
    "![](images\\cm_1.PNG)\n",
    "\n",
    "> It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n",
    "\n",
    "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.\n",
    "\n",
    "![](images\\cm_2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "![](images\\cm_3.PNG)\n",
    "\n",
    "\n",
    "1. __True Positive__: You predicted positive and it’s true.\n",
    "    You predicted that a woman is pregnant and she actually is.\n",
    "2. __True Negative__: You predicted negative and it’s true.\n",
    "    You predicted that a man is not pregnant and he actually is not.\n",
    "3. __False Positive: (Type 1 Error)__ You predicted positive and it’s false.\n",
    "    You predicted that a man is pregnant but he actually is not.\n",
    "4. __False Negative: (Type 2 Error)__ You predicted negative and it’s false.\n",
    "    You predicted that a woman is not pregnant but she actually is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\cm_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall - Out of all the actual positive values, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "### Precision - Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "### Accuracy - Out of all the classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "$$ Accuarcy = \\frac{TP+TN}{Actual} $$\n",
    "\n",
    "### F-measure \n",
    "It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n",
    "\n",
    "$$ F-measure = \\frac{2*Recall*Precision}{Recall + Precision} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "1. [Understanding Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)\n",
    "2. [What is a Confusion Matrix in Machine Learning](https://machinelearningmastery.com/confusion-matrix-machine-learning/)\n",
    "3. [Data school confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
