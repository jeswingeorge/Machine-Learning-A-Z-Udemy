{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Confusion Matrix and why you need it?\n",
    "\n",
    "A __confusion matrix__ is a table that is often used to describe the __performance of a classification model (or \"classifier\")__ on a set of test data for which the true values are known. \n",
    "\n",
    "Note: Used for __classification model__ and can be computed only when __true values are known__ (i.e, we have train-test split).\n",
    "\n",
    "The size of confusion matrix nxn depends on the number of labels n we have. So for, n - 3 labels we will have 3x3 matrix. For n = 10, we will have 10x10 matrix.\n",
    "\n",
    "For binary classifcation, it is a table with 4 different combinations of predicted and actual values.\n",
    "\n",
    "> Layout of confusion matrix can be different based on the software we are using.\n",
    "\n",
    "![](images\\cm_1.PNG)\n",
    "\n",
    "Imp pts:\n",
    "1. These TP, TN, FP and FN are whole numbers and not %ges or fractions.\n",
    "2. Must be binary classifiders and must be assignee +ve or -ve. For labels such as blue and green, take one as +ve and other as -ve.\n",
    "3. For >2 classes, do not use terminology TP, TN, FP and FN. As it will cause confusion.\n",
    "\n",
    "\n",
    "\n",
    "> It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True +ve, True -ve, False +ve and False -ve - Only for binary classifiers\n",
    "\n",
    "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy. For better understanding for yourself insert predicted between the 2 terms. Example: True _predicted_ positive, True _predicted_ negative, etc.\n",
    "\n",
    "Our aim for a classifier must be to always maximise TP and TN.\n",
    "\n",
    "![](images\\cm_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "![](images\\cm_3.PNG)\n",
    "\n",
    "\n",
    "1. __True Positive__: You predicted positive and it’s true.\n",
    "    You predicted that a woman is pregnant and she actually is.\n",
    "2. __True Negative__: You predicted negative and it’s true.\n",
    "    You predicted that a man is not pregnant and he actually is not.\n",
    "3. __False Positive: (Type 1 Error)__ You predicted positive and it’s false.\n",
    "    You predicted that a man is pregnant but he actually is not.\n",
    "4. __False Negative: (Type 2 Error)__ You predicted negative and it’s false.\n",
    "    You predicted that a woman is not pregnant but she actually is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\cm_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall or True +ve rate or Sensitivity\n",
    "When its actually True how often does it predict +ve.   \n",
    "Out of all the actual positive values, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$ or\n",
    "$$ = \\frac{TP}{total(true)}$$\n",
    "\n",
    "### True -ve rate or Specificity\n",
    "When its actually False, how often does it predict -ve.\n",
    "\n",
    "$$ Specificity = \\frac{TN}{FP+TN} $$ or\n",
    "$$ = \\frac{TN}{total(False)}$$\n",
    "\n",
    "### Precision \n",
    "When we predict +ve, how often it is correct?  \n",
    "Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "### Classification Accuracy \n",
    "Out of all the classes, how much we predicted correctly. It should be high as possible. Overall how often is the clasifier correct?\n",
    "\n",
    "$$ Accuarcy = \\frac{TP+TN}{total} $$\n",
    "\n",
    "For more than 2 classes, accuracy is calculated based on the sum of count preductions along the diagonal.\n",
    "\n",
    "\n",
    "### Misclassification rate \n",
    "Overall, how often is classifer wrong?\n",
    "\n",
    " $$ Misclassification-rate = \\frac{FP+FN}{total} $$ or,\n",
    " $$ = 1- accuracy$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### F-measure / F1-score\n",
    "It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n",
    "\n",
    "$$ F-measure = \\frac{2*Recall*Precision}{Recall + Precision} $$\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "![](images\\f1_1.PNG)\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "![](images\\f1_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Importance of classification matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Classification accuracy__ is the ratio of correct predictions to total predictions made.\n",
    "\n",
    "The main problem with classification accuracy is that it hides the detail you need to better understand the performance of your classification model. There are two examples where you are most likely to encounter this problem:\n",
    "\n",
    "1. When your data has more than 2 classes. With 3 or more classes you may get a classification accuracy of 80%, but you don’t know if that is because all classes are being predicted equally well or whether one or two classes are being neglected by the model.\n",
    "2. When your data does not have an even number of classes. You may achieve accuracy of 90% or more, but this is not a good score if 90 records for every 100 belong to one class and you can achieve this score by always predicting the most common class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearns layout of confusion matrix, actual values are aligned on the left hand side and the predicted values on the top. And labels are arranged in alphabetical order.\n",
    "\n",
    "![](images/cm_sk.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary case, we can extract true positives, etc as follows using `confusion_matrix(y_true, y_pred).ravel()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 1, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix and ROC curves are tool which aid in evaluation of a classifier, they are not the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## How to calculate the various metrics like recall and precision with 3 classes (eg: iris dataset)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,1,2,2,2]\n",
    "y_pred = [0,0,2,2,1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn calculates the precision, recall and f1-score for all the three values.\n",
    "\n",
    "- __class 0 recall is 1__ i.e, we have a zero in y_true amd it has been correctly predicted in y_pred.\n",
    "- __class 0 precision - When 0 was predicted, how often was it correct? - 50%__ i.e, 0 predicted twice but only 1 zero in true values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/conf_bus.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "1. [Understanding Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)\n",
    "2. [What is a Confusion Matrix in Machine Learning](https://machinelearningmastery.com/confusion-matrix-machine-learning/)\n",
    "3. [Data school confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
