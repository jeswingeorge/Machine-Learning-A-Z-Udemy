{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Confusion Matrix and why you need it?\n",
    "\n",
    "A __confusion matrix__ is a table that is often used to describe the __performance of a classification model (or \"classifier\")__ on a set of test data for which the true values are known. \n",
    "\n",
    "It is a table with 4 different combinations of predicted and actual values.\n",
    "\n",
    "![](images\\cm_1.PNG)\n",
    "\n",
    "> It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n",
    "\n",
    "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.\n",
    "\n",
    "![](images\\cm_2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just Remember, We describe predicted values as Positive and Negative and actual values as True and False.\n",
    "\n",
    "![](images\\cm_3.PNG)\n",
    "\n",
    "\n",
    "1. __True Positive__: You predicted positive and it’s true.\n",
    "    You predicted that a woman is pregnant and she actually is.\n",
    "2. __True Negative__: You predicted negative and it’s true.\n",
    "    You predicted that a man is not pregnant and he actually is not.\n",
    "3. __False Positive: (Type 1 Error)__ You predicted positive and it’s false.\n",
    "    You predicted that a man is pregnant but he actually is not.\n",
    "4. __False Negative: (Type 2 Error)__ You predicted negative and it’s false.\n",
    "    You predicted that a woman is not pregnant but she actually is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\cm_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall - Out of all the actual positive values, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "### Precision - Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "### Accuracy - Out of all the classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "$$ Accuarcy = \\frac{TP+TN}{Actual} $$\n",
    "\n",
    "### F-measure \n",
    "It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n",
    "\n",
    "$$ F-measure = \\frac{2*Recall*Precision}{Recall + Precision} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Importance of classification matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Classification accuracy__ is the ratio of correct predictions to total predictions made.\n",
    "\n",
    "The main problem with classification accuracy is that it hides the detail you need to better understand the performance of your classification model. There are two examples where you are most likely to encounter this problem:\n",
    "\n",
    "1. When your data has more than 2 classes. With 3 or more classes you may get a classification accuracy of 80%, but you don’t know if that is because all classes are being predicted equally well or whether one or two classes are being neglected by the model.\n",
    "2. When your data does not have an even number of classes. You may achieve accuracy of 90% or more, but this is not a good score if 90 records for every 100 belong to one class and you can achieve this score by always predicting the most common class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "1. [Understanding Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)\n",
    "2. [What is a Confusion Matrix in Machine Learning](https://machinelearningmastery.com/confusion-matrix-machine-learning/)\n",
    "3. [Data school confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
