# [Udemy Machine Learning A-Z course](https://www.udemy.com/machinelearning/)
[Download datasets for this course from website](https://www.superdatascience.com/pages/machine-learning)

This repository contains notes and assignments of this course.


## 1. Data pre-processing and Important Concepts
- [Data-preprocessing - Using sklearn and tutorials](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/Data-preprocessing%20-%20Using%20sklearn%20and%20tutorials.ipynb)  
  Topics covered: Missing data, Categorical data, Feature scaling, Splitting data into train and test set

- [Using sklearn's ColumnTransformer for Numerical and Categorical Data in Python](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/3.Use%20ColumnTransformer%20in%20SciKit%20instead%20of%20LabelEncoding%20and%20OneHotEncoding%20in%20ML.ipynb)  
  Topics covered: Using Column Transformer function 

- [Encoding categorical data - Ordinal and OneHotEncoding with pipeline and column tansformer](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/4.Encoding_categorical_features.ipynb)

- [Data-preprocessing - Using Pandas and my trials](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/Data-preprocessing%20-%20Using%20Pandas%20and%20my%20trials.ipynb)  
  Topics covered: Using pandas for Missing data, Categorical data
  
- [Transforming data for normality](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/5.Transforming-data-for-normality.ipynb)  
Topics covered: Box-cox transform

- __Notes on association analysis__:   
    Topics covered: Covariance, Correlation and their types, Association between categorical variables - Chi-squared test, Association between two binary variables- Phi-coefficient, Association between 2 nominal variables - Crammers' V, Theilâ€™s U, The Simpson Paradox, Market Basket Analysis - Apriori Algorithm.

- [Dealing with Multicolinearity](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/6.dealing_with_multicolinearity.ipynb)  
Topics covered: Identifying multiclinearity, Variance Inflation factor (VIF)

- [Dimensionality Reduction (PCA)](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/7.Principal-Component-Analysis-%28PCA%29.ipynb)  
Topics covered: Principal component Analysis 



## 2. Regression
- [Regression Introduction](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/3.Intro/Regression%20Intro.ipynb)  
  Topics covered: Regression definition and types
- [Simple Linear Regression](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/4.%20Simple%20Linear%20Regression/simple_linear_regression.ipynb)  
  Topics covered: Simple linear regression Introduction
- [Simple Linear Regression - Notes-2](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/4.%20Simple%20Linear%20Regression/Simple_Linear_reg-Notes-2.ipynb)  
 Topics covered: Assumptions of Linear Regression and their validation, Coefficient of determination $R^{2}$ and adjusted $R^{2}$.OLS function, Finding adj Rsq and Rsq fron the OLS mdoels.
- [Multiple Linear Regression-1](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/5.Multiple%20Linear%20Regression/2.%20Multiple%20regression.ipynb)  
	Topics covered: Creating dummy variables, Dummy Variable Trap, P-value in regression analysis and coefficients
- [Multiple Linear Regression-2: 5 methods of building a model](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/5.Multiple%20Linear%20Regression/1.%20Building%20a%20model%20%28step%20by%20step%29.ipynb)  
 Topics covered: All-in, Backward Elimination, Forward selection, Bidirectional Elimination, Score comparison/All possible models
- [Multiple linear regression implementation with backward elimination](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/5.Multiple%20Linear%20Regression/3.multi_linear_reg_backward_elimination.ipynb)
- [Multiple linear regression - Assumptions]()
- [Polynomial regression](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/6.%20Polynomial%20Regression/polynomial_regression.ipynb)  
  Topics covered - why polynomial regression is called linear, Implementing Polynomial regression
- [Support Vector Regression(SVR)](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/7.Support%20Vector%20Regression%20SVR/support_vector_regression.ipynb)
- [Decision Tree Regression](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/8.%20Decision%20Tree%20Regression/decision_trees.ipynb)
- [Random Forest Regression](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/9.Random%20Forest%20Regression/random_forest_regression.ipynb)
- [Ridge, Lasso and Elastic-net regression](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/10.Ridge%2Classo_elastic_regression.ipynb)
    Regularization, L1/L2 regularization
- [Evaluating regression models performance](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/2.Regression/10.Evaluating%20Regression%20Models%20Performance/regression_evaluation_methods.ipynb)  
  Topics covered:
  	1. R-squared, Adjusted R-sq
  	2. Reference material IMP: Why using residual plots is important even when value of R-squared is high/low?
    3. MSE, RMSE,MAPE, MAE
- [Measures of spread - Skewness and Kurtosis](https://nbviewer.jupyter.org/github/jeswingeorge/Python-DS-notes/blob/master/Statistics/1.measures_of_shape.ipynb)


## 3. Classification

- [Logistic Regression - Intro]()
- [Logistic Regression]()
- [Confusion Matrix]()
    Confusion matrix, accuracy, recall, precision and F-measure.

## Feature Selection

- [Choosing a Feature Selection Method For ML](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/10.Feature_selection.ipynb)

- [Feature Selection using SelectKBest & Recursive Feature Elimination](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/1.Data%20Preprocessing/9.Feature%20Selection%20using%20SelectKBest%20%26%20Recursive%20Feature%20Elimination.ipynb)
 


## Model Selection

- [Cross-validation Introduction](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/11.Model-Selection/cross_validation_data_school.ipynb)

- [K-Fold cross validation](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/11.Model-Selection/k-fold_cross_validation.ipynb)
	- Parameter and a Hyperparameter
	- k-Fold cross validation
	- Bias Variance Trade-off
	- Bulls eye diagram
	- 5 reasons for using cross-validation
 
-  [Grid Search](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/11.Model-Selection/grid-search-cv.ipynb)
- [GridSearch vs RandomizedSearch](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/11.Model-Selection/Grid-vs-Random-Search-Hyperparameter-tuning.ipynb)

- [Maths behind RandomizedSearch](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/11.Model-Selection/Randomized_search.ipynb)


## Ensemble Learning

- [Different methods in ensemble learning - AV blog notes](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/13.Ensemble/1.Ensemble%20learning.ipynb)
- [Gradient Boosting Introduction](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/12.%20Gradient%20Boosting/1.Gradient%20Boost%20-%20Intro.ipynb)
- [Math behind gradient boosting](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/12.%20Gradient%20Boosting/2.%20Math%20behind%20XGBoost.ipynb)
- [Implementing Gradient Boosting Python](https://nbviewer.jupyter.org/github/jeswingeorge/Machine-Learning-A-Z-Udemy/blob/master/12.%20Gradient%20Boosting/3.Implementing_XGBoost_Python.ipynb)
- [Catboost Implementation]()
- [Gradient Boosting Hyperparameter tuning]()



